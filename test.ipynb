{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3d0e6766",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Auto reloads modules when they change\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0317c974",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.dataset import wheatDataset\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e1ac49e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>width</th>\n",
       "      <th>height</th>\n",
       "      <th>bbox</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b6ab77fd7</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>[834.0, 222.0, 56.0, 36.0]</td>\n",
       "      <td>usask_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b6ab77fd7</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>[226.0, 548.0, 130.0, 58.0]</td>\n",
       "      <td>usask_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b6ab77fd7</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>[377.0, 504.0, 74.0, 160.0]</td>\n",
       "      <td>usask_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b6ab77fd7</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>[834.0, 95.0, 109.0, 107.0]</td>\n",
       "      <td>usask_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b6ab77fd7</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>[26.0, 144.0, 124.0, 117.0]</td>\n",
       "      <td>usask_1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    image_id  width  height                         bbox   source\n",
       "0  b6ab77fd7   1024    1024   [834.0, 222.0, 56.0, 36.0]  usask_1\n",
       "1  b6ab77fd7   1024    1024  [226.0, 548.0, 130.0, 58.0]  usask_1\n",
       "2  b6ab77fd7   1024    1024  [377.0, 504.0, 74.0, 160.0]  usask_1\n",
       "3  b6ab77fd7   1024    1024  [834.0, 95.0, 109.0, 107.0]  usask_1\n",
       "4  b6ab77fd7   1024    1024  [26.0, 144.0, 124.0, 117.0]  usask_1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_dir = \"./data/global-wheat-detection/train.csv\"\n",
    "image_dir = \"./data/global-wheat-detection/train\"\n",
    "image_size = 512\n",
    "\n",
    "df = pd.read_csv(csv_dir)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "975a13df",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = wheatDataset(df, image_dir, image_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f9d0464b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'boxes': tensor([[   0.,   69.,   37.,  130.],\n",
      "        [   0.,  129.,   66.,  188.],\n",
      "        [   6.,  174.,   76.,  248.],\n",
      "        [   0.,  734.,   63.,  818.],\n",
      "        [ 189.,  844.,  256.,  915.],\n",
      "        [ 183.,  791.,  265.,  858.],\n",
      "        [ 198.,  725.,  284.,  795.],\n",
      "        [ 171.,  611.,  247.,  701.],\n",
      "        [ 169.,  561.,  238.,  623.],\n",
      "        [ 220.,  523.,  293.,  606.],\n",
      "        [  45.,  357.,  165.,  478.],\n",
      "        [ 344.,  314.,  435.,  397.],\n",
      "        [ 330.,  660.,  408.,  725.],\n",
      "        [ 327.,  777.,  406.,  850.],\n",
      "        [ 331.,  721.,  404.,  776.],\n",
      "        [ 428.,  705.,  519.,  766.],\n",
      "        [ 404.,  756.,  476.,  857.],\n",
      "        [ 489.,  751.,  568.,  839.],\n",
      "        [ 555.,  754.,  625.,  837.],\n",
      "        [ 581.,  701.,  690.,  750.],\n",
      "        [ 626.,  625.,  714.,  698.],\n",
      "        [ 596.,  795.,  682.,  908.],\n",
      "        [ 640.,  747.,  707.,  814.],\n",
      "        [ 291.,  861.,  350.,  928.],\n",
      "        [ 744.,  789.,  835.,  862.],\n",
      "        [ 835.,  679.,  925.,  765.],\n",
      "        [ 848.,  615.,  938.,  691.],\n",
      "        [ 670.,  308.,  746.,  378.],\n",
      "        [ 761.,  343.,  837.,  420.],\n",
      "        [ 815.,  366.,  910.,  438.],\n",
      "        [ 716.,  267.,  829.,  340.],\n",
      "        [ 452.,  206.,  522.,  281.],\n",
      "        [ 490.,  126.,  554.,  206.],\n",
      "        [ 521.,   91.,  584.,  164.],\n",
      "        [ 596.,   90.,  681.,  176.],\n",
      "        [ 593.,  169.,  691.,  236.],\n",
      "        [ 647.,  199.,  740.,  266.],\n",
      "        [ 720.,   10.,  823.,   99.],\n",
      "        [ 755.,   66.,  828.,  137.],\n",
      "        [ 810.,  144.,  893.,  229.],\n",
      "        [ 889.,   97.,  964.,  220.],\n",
      "        [ 947.,  125., 1024.,  216.],\n",
      "        [ 986.,  314., 1024.,  391.],\n",
      "        [ 602.,  115.,  676.,  188.],\n",
      "        [  77.,  697.,  152.,  740.],\n",
      "        [  50.,  801.,  109.,  853.],\n",
      "        [ 997.,  601., 1024.,  671.]]), 'area': tensor([ 2257.,  3894.,  5180.,  5292.,  4757.,  5494.,  6020.,  6840.,  4278.,\n",
      "         6059., 14520.,  7553.,  5070.,  5767.,  4015.,  5551.,  7272.,  6952.,\n",
      "         5810.,  5341.,  6424.,  9718.,  4489.,  3953.,  6643.,  7740.,  6840.,\n",
      "         5320.,  5852.,  6840.,  8249.,  5250.,  5120.,  4599.,  7310.,  6566.,\n",
      "         6231.,  9167.,  5183.,  7055.,  9225.,  7007.,  2926.,  5402.,  3225.,\n",
      "         3068.,  1890.]), 'labels': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), 'iscrowd': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}\n"
     ]
    }
   ],
   "source": [
    "img, dict = dataset[0]\n",
    "print(dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fdd4ffe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape: torch.Size([3, 1024, 1024])\n",
      "Image dtype: torch.float32\n",
      "Boxes shape: torch.Size([47, 4])\n",
      "Labels shape: torch.Size([47])\n",
      "Area shape: torch.Size([47])\n",
      "Image value range: 0.000 to 1.000\n"
     ]
    }
   ],
   "source": [
    "# Test getting one sample\n",
    "image, target = dataset[0]\n",
    "\n",
    "# Check shapes and types\n",
    "print(f\"Image shape: {image.shape}\")  # Should be (3, H, W)\n",
    "print(f\"Image dtype: {image.dtype}\")  # Should be torch.float32\n",
    "print(f\"Boxes shape: {target['boxes'].shape}\")  # Should be (N, 4)\n",
    "print(f\"Labels shape: {target['labels'].shape}\")  # Should be (N,)\n",
    "print(f\"Area shape: {target['area'].shape}\")  # Should be (N,)\n",
    "print(f\"Image value range: {image.min():.3f} to {image.max():.3f}\")  # Should be [0, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9543d4c",
   "metadata": {},
   "source": [
    "Steps:\n",
    "1. Prepare data with dataset\n",
    "2. Train test split\n",
    "3. Dataloader setup\n",
    "4. Model setup\n",
    "5. train loop\n",
    "6. Validation\n",
    "7. Inference"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle-wheat-detection",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

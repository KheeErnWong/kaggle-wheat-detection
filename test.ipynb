{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d0e6766",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auto reloads modules when they change\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a235f5dc",
   "metadata": {},
   "source": [
    "```Flow```\n",
    "1. EDA\n",
    "2. Decide model to use\n",
    "3. Prepare dataset to fit model format and do image augmentations\n",
    "4. Do a train/test split\n",
    "5. Use dataloader to split dataset into batches for feeding into model\n",
    "6. Initialize model\n",
    "7. Train model\n",
    "8. Validate model\n",
    "9. Inference (predict on test.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0317c974",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.dataset import wheatDataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from torchvision.models.detection import (\n",
    "    fasterrcnn_resnet50_fpn,\n",
    "    FasterRCNN_ResNet50_FPN_Weights,\n",
    ")\n",
    "from torchvision.models import ResNet50_Weights\n",
    "import torch.optim as optim\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1ac49e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>width</th>\n",
       "      <th>height</th>\n",
       "      <th>bbox</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b6ab77fd7</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>[834.0, 222.0, 56.0, 36.0]</td>\n",
       "      <td>usask_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b6ab77fd7</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>[226.0, 548.0, 130.0, 58.0]</td>\n",
       "      <td>usask_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b6ab77fd7</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>[377.0, 504.0, 74.0, 160.0]</td>\n",
       "      <td>usask_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b6ab77fd7</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>[834.0, 95.0, 109.0, 107.0]</td>\n",
       "      <td>usask_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b6ab77fd7</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>[26.0, 144.0, 124.0, 117.0]</td>\n",
       "      <td>usask_1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    image_id  width  height                         bbox   source\n",
       "0  b6ab77fd7   1024    1024   [834.0, 222.0, 56.0, 36.0]  usask_1\n",
       "1  b6ab77fd7   1024    1024  [226.0, 548.0, 130.0, 58.0]  usask_1\n",
       "2  b6ab77fd7   1024    1024  [377.0, 504.0, 74.0, 160.0]  usask_1\n",
       "3  b6ab77fd7   1024    1024  [834.0, 95.0, 109.0, 107.0]  usask_1\n",
       "4  b6ab77fd7   1024    1024  [26.0, 144.0, 124.0, 117.0]  usask_1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_dir = \"./data/global-wheat-detection/train.csv\"\n",
    "image_dir = \"./data/global-wheat-detection/train\"\n",
    "image_size = 1024\n",
    "\n",
    "df = pd.read_csv(csv_dir)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68fafa22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train images: 2698\n",
      "Val images: 675\n",
      "Train bbox rows: 118371\n",
      "Val bbox rows: 29422\n",
      "Avg boxes per image (train): 43.87\n",
      "Avg boxes per image (val): 43.59\n"
     ]
    }
   ],
   "source": [
    "# Split image_id 80/20 for for train/val set\n",
    "train_ids, val_ids = train_test_split(\n",
    "    df[\"image_id\"].unique().tolist(), test_size=0.2, random_state=42, shuffle=True\n",
    ")\n",
    "\n",
    "# Get corresponding image data into dataframe\n",
    "train_df = df.loc[df[\"image_id\"].isin(train_ids)]\n",
    "val_df = df.loc[df[\"image_id\"].isin(val_ids)]\n",
    "\n",
    "# Check if split correctly\n",
    "print(f\"Train images: {len(train_ids)}\")\n",
    "print(f\"Val images: {len(val_ids)}\")\n",
    "print(f\"Train bbox rows: {len(train_df)}\")\n",
    "print(f\"Val bbox rows: {len(val_df)}\")\n",
    "\n",
    "# Check average boxes per image\n",
    "print(f\"Avg boxes per image (train): {len(train_df) / len(train_ids):.2f}\")\n",
    "print(f\"Avg boxes per image (val): {len(val_df) / len(val_ids):.2f}\")\n",
    "\n",
    "assert set(train_ids).isdisjoint(set(val_ids)), \"Data leakage detected!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5122e0e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train image shape: torch.Size([3, 1024, 1024])\n",
      "Train dataset boxes shape: torch.Size([24, 4])\n",
      "Val image shape: torch.Size([3, 1024, 1024])\n",
      "Val dataset boxes shape: torch.Size([47, 4])\n"
     ]
    }
   ],
   "source": [
    "# Return tensors and target dict for each dataset\n",
    "train_dataset = wheatDataset(train_df, image_dir, image_size, mode=\"train\")\n",
    "val_dataset = wheatDataset(val_df, image_dir, image_size, mode=\"validation\")\n",
    "\n",
    "train_image, train_target = train_dataset[0]\n",
    "val_image, val_target = val_dataset[0]\n",
    "\n",
    "# Check if correct\n",
    "print(f\"Train image shape: {train_image.shape}\")\n",
    "print(f\"Train dataset boxes shape: {train_target['boxes'].shape}\")\n",
    "\n",
    "print(f\"Val image shape: {val_image.shape}\")\n",
    "print(f\"Val dataset boxes shape: {val_target['boxes'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c7e796be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom collate function since bbox tensors of different shape cannot be stacked\n",
    "def collate_fn(batch):\n",
    "    # Separating batch contents from dataloader\n",
    "    images = [item[0] for item in batch]\n",
    "    targets = [item[1] for item in batch]\n",
    "\n",
    "    # Stack images\n",
    "    images = torch.stack(images)\n",
    "\n",
    "    # Return tuple with stacked images but non-stacked targets\n",
    "    return (images, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07ad76a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 1024, 1024])\n",
      "2\n",
      "<class 'list'>\n",
      "2\n",
      "dict_keys(['boxes', 'area', 'labels', 'iscrowd'])\n",
      "torch.Size([50, 4])\n"
     ]
    }
   ],
   "source": [
    "# Data must be in the format that model wants, images have to be augmented\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=2, shuffle=True, collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(val_dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "for images, targets in train_loader:\n",
    "    print(images.shape)\n",
    "    print(len(images))\n",
    "    print(type(targets))\n",
    "    print(len(targets))\n",
    "    print(targets[0].keys())\n",
    "    print(targets[0][\"boxes\"].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e6d3199",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.9.0\n",
      "MPS available: True\n"
     ]
    }
   ],
   "source": [
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"MPS available: {torch.backends.mps.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "566da1eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FasterRCNN(\n",
       "  (transform): GeneralizedRCNNTransform(\n",
       "      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
       "      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n",
       "  )\n",
       "  (backbone): BackboneWithFPN(\n",
       "    (body): IntermediateLayerGetter(\n",
       "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "      (bn1): FrozenBatchNorm2d(64, eps=1e-05)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      (layer1): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=1e-05)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=1e-05)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=1e-05)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=1e-05)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=1e-05)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=1e-05)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer2): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=1e-05)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=1e-05)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(512, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=1e-05)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=1e-05)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=1e-05)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=1e-05)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=1e-05)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=1e-05)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer3): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(1024, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (4): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (5): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer4): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=1e-05)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=1e-05)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(2048, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=1e-05)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=1e-05)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=1e-05)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=1e-05)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (fpn): FeaturePyramidNetwork(\n",
       "      (inner_blocks): ModuleList(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (2): Conv2dNormActivation(\n",
       "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (layer_blocks): ModuleList(\n",
       "        (0-3): 4 x Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (extra_blocks): LastLevelMaxPool()\n",
       "    )\n",
       "  )\n",
       "  (rpn): RegionProposalNetwork(\n",
       "    (anchor_generator): AnchorGenerator()\n",
       "    (head): RPNHead(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (roi_heads): RoIHeads(\n",
       "    (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\n",
       "    (box_head): TwoMLPHead(\n",
       "      (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n",
       "      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    )\n",
       "    (box_predictor): FastRCNNPredictor(\n",
       "      (cls_score): Linear(in_features=1024, out_features=2, bias=True)\n",
       "      (bbox_pred): Linear(in_features=1024, out_features=8, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set device as mps\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# Initialize model\n",
    "model = fasterrcnn_resnet50_fpn(\n",
    "    weights=None, weights_backbone=ResNet50_Weights.DEFAULT, num_classes=2\n",
    ")\n",
    "\n",
    "# Bring model to device\n",
    "model = model.to(device)\n",
    "\n",
    "# Set model to train mode\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fb6718bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total loss: 1.7287588119506836\n",
      "Loss components: {'loss_classifier': tensor(0.7036, grad_fn=<NllLossBackward0>), 'loss_box_reg': tensor(0.1418, grad_fn=<DivBackward0>), 'loss_objectness': tensor(0.7361, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>), 'loss_rpn_box_reg': tensor(0.1472, grad_fn=<DivBackward0>)}\n",
      "Backward pass completed\n"
     ]
    }
   ],
   "source": [
    "# Get one iteration\n",
    "images, targets = next(iter(train_loader))\n",
    "\n",
    "# Send image and targets to device\n",
    "images = images.to(device)\n",
    "\n",
    "# Nested list comprehension that moves all tensors in target dict to device\n",
    "targets = [\n",
    "    {key: value.to(device) for key, value in target.items()} for target in targets\n",
    "]\n",
    "\n",
    "# Forward pass\n",
    "loss_dict = model(images, targets)\n",
    "\n",
    "# Calculate total loss\n",
    "losses = sum(loss for loss in loss_dict.values())\n",
    "print(f\"Total loss: {losses.item()}\")\n",
    "print(f\"Loss components: {loss_dict}\")\n",
    "\n",
    "# Backward pass\n",
    "losses.backward()\n",
    "print(\"Backward pass completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f93d779e",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 2\n",
    "lr = 0.0001\n",
    "print_freq = 50\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23794c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 / 2\n",
      "Batch index: 0/1349\n",
      "Loss: 2.3506\n",
      "Batch index: 50/1349\n",
      "Loss: 1.4337\n",
      "Batch index: 100/1349\n",
      "Loss: 1.6045\n",
      "Batch index: 150/1349\n",
      "Loss: 1.4779\n",
      "Batch index: 200/1349\n",
      "Loss: 1.1758\n",
      "Batch index: 250/1349\n",
      "Loss: 1.1516\n",
      "Batch index: 300/1349\n",
      "Loss: 1.2481\n",
      "Batch index: 350/1349\n",
      "Loss: 0.9951\n",
      "Batch index: 400/1349\n",
      "Loss: 1.0389\n",
      "Batch index: 450/1349\n",
      "Loss: 0.9663\n",
      "Batch index: 500/1349\n",
      "Loss: 1.0426\n",
      "Batch index: 550/1349\n",
      "Loss: 1.0050\n",
      "Batch index: 600/1349\n",
      "Loss: 1.0232\n",
      "Batch index: 650/1349\n",
      "Loss: 0.8992\n",
      "Batch index: 700/1349\n",
      "Loss: 0.9892\n",
      "Batch index: 750/1349\n",
      "Loss: 0.9082\n",
      "Batch index: 800/1349\n",
      "Loss: 0.9075\n",
      "Batch index: 850/1349\n",
      "Loss: 0.8665\n",
      "Batch index: 900/1349\n",
      "Loss: 0.8570\n",
      "Batch index: 950/1349\n",
      "Loss: 0.8995\n",
      "Batch index: 1000/1349\n",
      "Loss: 0.8605\n",
      "Batch index: 1050/1349\n",
      "Loss: 1.1186\n",
      "Batch index: 1100/1349\n",
      "Loss: 0.9336\n",
      "Batch index: 1150/1349\n",
      "Loss: 0.9503\n",
      "Batch index: 1200/1349\n",
      "Loss: 0.9945\n",
      "Batch index: 1250/1349\n",
      "Loss: 0.8562\n",
      "Batch index: 1300/1349\n",
      "Loss: 1.0820\n",
      "Epoch 2 / 2\n",
      "Batch index: 0/1349\n",
      "Loss: 0.8416\n",
      "Batch index: 50/1349\n",
      "Loss: 0.7822\n",
      "Batch index: 100/1349\n",
      "Loss: 1.0222\n",
      "Batch index: 150/1349\n",
      "Loss: 0.9340\n",
      "Batch index: 200/1349\n",
      "Loss: 0.7405\n",
      "Batch index: 250/1349\n",
      "Loss: 0.9703\n",
      "Batch index: 300/1349\n",
      "Loss: 0.6375\n",
      "Batch index: 350/1349\n",
      "Loss: 0.7784\n",
      "Batch index: 400/1349\n",
      "Loss: 0.8625\n",
      "Batch index: 450/1349\n",
      "Loss: 0.7691\n",
      "Batch index: 500/1349\n",
      "Loss: 0.9045\n",
      "Batch index: 550/1349\n",
      "Loss: 0.8263\n",
      "Batch index: 600/1349\n",
      "Loss: 0.7444\n",
      "Batch index: 650/1349\n",
      "Loss: 0.7126\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch + 1} / {num_epochs}\")\n",
    "\n",
    "    # Set model to training mode\n",
    "    model.train()\n",
    "\n",
    "    for batch_idx, (images, targets) in enumerate(train_loader):\n",
    "        # Load train_data into device\n",
    "        images = images.to(device)\n",
    "        targets = [\n",
    "            {key: value.to(device) for key, value in target.items()}\n",
    "            for target in targets\n",
    "        ]\n",
    "\n",
    "        # Clear gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        loss_dict = model(images, targets)\n",
    "\n",
    "        # Convert loss dict to scalar\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "        # Backward pass to compute gradients\n",
    "        losses.backward()\n",
    "\n",
    "        # Gradient update with optimizer\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print batch_idx\n",
    "        if batch_idx % print_freq == 0:\n",
    "            print(f\"Batch index: {batch_idx}/{len(train_loader)}\")\n",
    "            print(f\"Loss: {losses.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9e5df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation loop\n",
    "val_losses = 0.0\n",
    "\n",
    "# Put model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, targets in val_loader:\n",
    "        images = images.to(device)\n",
    "        targets = [\n",
    "            {key: value.to(device) for key, value in target.items()}\n",
    "            for target in targets\n",
    "        ]\n",
    "\n",
    "        # Compute loss\n",
    "        loss_dict = model(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "        # Append loss\n",
    "        val_losses += losses.item()\n",
    "\n",
    "avg_val_loss = val_losses / len(val_loader)\n",
    "print(f\"Val_loss: {avg_val_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df460aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine together\n",
    "# Make directory to save models\n",
    "os.makedirs(\"./checkpoints/models\", exist_ok=True)\n",
    "\n",
    "\n",
    "## Add tqdm next time\n",
    "optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "# Initialize best val loss\n",
    "best_val_loss = float(\"inf\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch + 1} / {num_epochs}\")\n",
    "\n",
    "    # Set model to train mode\n",
    "    model.train()\n",
    "\n",
    "    ## Initialize tracking variables\n",
    "    train_losses = 0.0\n",
    "    val_losses = 0.0\n",
    "\n",
    "    for batch_idx, (images, targets) in enumerate(train_loader):\n",
    "        # Load train_data into device\n",
    "        images = images.to(device)\n",
    "        targets = [\n",
    "            {key: value.to(device) for key, value in target.items()}\n",
    "            for target in targets\n",
    "        ]\n",
    "\n",
    "        # Clear gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        loss_dict = model(images, targets)\n",
    "\n",
    "        # Convert loss dict to scalar\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "        # Backward pass to compute gradients\n",
    "        losses.backward()\n",
    "\n",
    "        # Gradient update with optimizer\n",
    "        optimizer.step()\n",
    "\n",
    "        # Calculate train tracking metrics\n",
    "        train_losses += losses.item()\n",
    "\n",
    "        # Print batch_idx and loss\n",
    "        if batch_idx % print_freq == 0:\n",
    "            print(f\"Batch index: {batch_idx}/{len(train_loader)}\")\n",
    "            print(f\"Batch loss: {losses.item():.4f}\")\n",
    "\n",
    "    # Put model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, targets in val_loader:\n",
    "            images = images.to(device)\n",
    "            targets = [\n",
    "                {key: value.to(device) for key, value in target.items()}\n",
    "                for target in targets\n",
    "            ]\n",
    "\n",
    "            # Compute loss\n",
    "            loss_dict = model(images, targets)\n",
    "            losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "            # Append loss\n",
    "            val_losses += losses.item()\n",
    "\n",
    "    # Calculate tracking metrics\n",
    "    avg_train_loss = train_losses / len(train_loader)\n",
    "    avg_val_loss = val_losses / len(val_loader)\n",
    "\n",
    "    # Print metrics\n",
    "    print(f\"Train_loss: {avg_train_loss:.4f}\")\n",
    "    print(f\"Val_loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    # Save model if avg_val_loss improves\n",
    "    ## Save to mlflow next time\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        torch.save(\n",
    "            {\n",
    "                \"epoch\": epoch,\n",
    "                \"model_save_dict\": model.state_dict(),\n",
    "                \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                \"train_loss\": avg_train_loss,\n",
    "                \"val_loss\": avg_val_loss,\n",
    "            },\n",
    "            \"./checkpoints/models/best_model.pth\",\n",
    "        )\n",
    "        print(f\"Saved best model (val_loss: {avg_val_loss:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aad3c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference (testing first)\n",
    "\n",
    "# Get one batch from val_loader\n",
    "images, targets = next(iter(val_loader))\n",
    "\n",
    "# Set to eval mode\n",
    "model.eval()\n",
    "\n",
    "# Inference\n",
    "with torch.no_grad():\n",
    "    predictions = model(images)\n",
    "\n",
    "# Inspect output\n",
    "print(f\"Number of images: {len(predictions)}\")\n",
    "print(f\"First prediction keys: {predictions[0].keys()}\")\n",
    "print(f\"Boxes shape: {predictions[0]['boxes'].shape}\")\n",
    "print(f\"Scores: {predictions[0]['scores']}\")\n",
    "print(f\"Labels: {predictions[0]['labels']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a260764c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a9543d4c",
   "metadata": {},
   "source": [
    "Steps:\n",
    "1. Prepare data with dataset\n",
    "2. Train test split\n",
    "3. Dataloader setup\n",
    "4. Model setup\n",
    "5. train loop\n",
    "6. Validation\n",
    "7. Inference"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle-wheat-detection",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
